{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 辽宁高速公路裂缝检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::002224604296:role/service-role/AmazonSageMaker-ExecutionRole-20200402T124851\n",
      "CPU times: user 834 ms, sys: 182 ms, total: 1.02 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    " \n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-002224604296\n"
     ]
    }
   ],
   "source": [
    "bucket = sess.default_bucket()  \n",
    "prefix = 'semantic-segmentation-demo'\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备训练镜像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/semantic-segmentation:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "training_image = get_image_uri(sess.boto_region_name, 'semantic-segmentation', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Num Train Images = 170\n",
      " Num Validation Images = 31\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "num_training_samples=len(glob.glob1('train',\"*.jpg\"))\n",
    "\n",
    "print ( ' Num Train Images = ' + str(num_training_samples))\n",
    "assert num_training_samples == len(glob.glob1('train_annotation',\"*.png\"))\n",
    "\n",
    "print ( ' Num Validation Images = ' + str(len(glob.glob1('validation',\"*.jpg\"))))\n",
    "assert len(glob.glob1('validation',\"*.jpg\")) == len(glob.glob1('validation_annotation',\"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create channel names for the s3 bucket.\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "train_annotation_channel = prefix + '/train_annotation'\n",
    "validation_annotation_channel = prefix + '/validation_annotation'\n",
    "# test_input_channel = prefix + '/test_input_20200511'\n",
    "test_input_channel = prefix + '/test_input_liaoning_sample'\n",
    "test_output_channel = prefix + '/test_output'\n",
    "# label_map_channel = prefix + '/label_map'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上传数据到S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.3 s, sys: 354 ms, total: 3.65 s\n",
      "Wall time: 1min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-002224604296/semantic-segmentation-demo/validation_annotation'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# upload the appropraite directory up to s3 respectively for all directories.\n",
    "sess.upload_data(path='train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='validation', bucket=bucket, key_prefix=validation_channel)\n",
    "sess.upload_data(path='train_annotation', bucket=bucket, key_prefix=train_annotation_channel)\n",
    "sess.upload_data(path='validation_annotation', bucket=bucket, key_prefix=validation_annotation_channel)\n",
    "# sess.upload_data(path='train_label_map.json', bucket=bucket, key_prefix=label_map_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-002224604296/semantic-segmentation-demo/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sagemaker estimator object.\n",
    "ss_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count = 1, \n",
    "                                         train_instance_type = 'ml.p3.2xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         output_path = s3_output_location,\n",
    "                                         base_job_name = 'ss-notebook-demo',\n",
    "                                         sagemaker_session = sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters \n",
    "ss_model.set_hyperparameters(backbone='resnet-101', # This is the encoder. Other option is resnet-50\n",
    "                             algorithm='deeplab', # This is the decoder. Other option is 'psp' and 'deeplab'                             \n",
    "                             use_pretrained_model='True', # Use the pre-trained model.\n",
    "                             crop_size=240, # Size of image random crop.                             \n",
    "                             num_classes=2, # Pascal has 21 classes. This is a mandatory parameter.\n",
    "                             epochs=20, # Number of epochs to run.\n",
    "                             learning_rate=0.0001,                             \n",
    "                             optimizer='adam', # Other options include 'adam', 'rmsprop', 'nag', 'adagrad'.\n",
    "                             lr_scheduler='poly', # Other options include 'cosine' and 'step'.                           \n",
    "                             mini_batch_size=16, # Setup some mini batch size.\n",
    "                             validation_mini_batch_size=16,\n",
    "                             early_stopping=True, # Turn on early stopping. If OFF, other early stopping parameters are ignored.\n",
    "                             early_stopping_patience=2, # Tolerate these many epochs if the mIoU doens't increase.\n",
    "                             early_stopping_min_epochs=10, # No matter what, run these many number of epochs.                             \n",
    "                             num_training_samples=num_training_samples) # This is a mandatory parameter, 1464 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full bucket names\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)\n",
    "s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)\n",
    "s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)\n",
    "\n",
    "distribution = 'FullyReplicated'\n",
    "# Create sagemaker s3_input objects\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution=distribution, \n",
    "                                        content_type='image/jpeg', s3_data_type='S3Prefix')\n",
    "train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution=distribution, \n",
    "                                        content_type='image/png', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, \n",
    "                 'validation': validation_data,\n",
    "                 'train_annotation': train_annotation, \n",
    "                 'validation_annotation':validation_annotation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-29 01:48:57 Starting - Starting the training job...\n",
      "2020-04-29 01:48:58 Starting - Launching requested ML instances......\n",
      "2020-04-29 01:50:07 Starting - Preparing the instances for training......\n",
      "2020-04-29 01:51:20 Downloading - Downloading input data\n",
      "2020-04-29 01:51:20 Training - Downloading the training image........\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:39 INFO 140228826310464] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'syncbn': u'False', u'gamma2': u'0.9', u'gamma1': u'0.9', u'early_stopping_min_epochs': u'5', u'epochs': u'10', u'_workers': u'16', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0001', u'crop_size': u'240', u'use_pretrained_model': u'True', u'_aux_weight': u'0.5', u'_hybrid': u'False', u'_augmentation_type': u'default', u'lr_scheduler': u'poly', u'early_stopping_patience': u'4', u'momentum': u'0.9', u'optimizer': u'sgd', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'backbone': u'resnet-50', u'validation_mini_batch_size': u'16', u'_aux_loss': u'True', u'mini_batch_size': u'16', u'early_stopping': u'False', u'algorithm': u'fcn', u'_logging_frequency': u'20', u'num_training_samples': u'8', u'_kvstore': u'device', u'precision_dtype': u'float32'}\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:39 INFO 140228826310464] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.0001', u'optimizer': u'adam', u'algorithm': u'deeplab', u'lr_scheduler': u'poly', u'use_pretrained_model': u'True', u'backbone': u'resnet-101', u'early_stopping_min_epochs': u'10', u'epochs': u'20', u'validation_mini_batch_size': u'16', u'num_training_samples': u'170', u'num_classes': u'2', u'mini_batch_size': u'16', u'early_stopping_patience': u'2', u'early_stopping': u'True', u'crop_size': u'240'}\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:39 INFO 140228826310464] Final configuration: {u'syncbn': u'False', u'gamma2': u'0.9', u'gamma1': u'0.9', u'early_stopping_min_epochs': u'10', u'epochs': u'20', u'_workers': u'16', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0001', u'crop_size': u'240', u'use_pretrained_model': u'True', u'_aux_weight': u'0.5', u'_hybrid': u'False', u'_augmentation_type': u'default', u'lr_scheduler': u'poly', u'num_classes': u'2', u'early_stopping_patience': u'2', u'momentum': u'0.9', u'optimizer': u'adam', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.0001', u'backbone': u'resnet-101', u'validation_mini_batch_size': u'16', u'_aux_loss': u'True', u'mini_batch_size': u'16', u'early_stopping': u'True', u'algorithm': u'deeplab', u'_logging_frequency': u'20', u'num_training_samples': u'170', u'_kvstore': u'device', u'precision_dtype': u'float32'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:39 INFO 140228826310464] Using default worker.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:39 INFO 140228826310464] font search path ['/opt/amazon/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/amazon/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/amazon/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:40 INFO 140228826310464] generated new fontManager\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:40 INFO 140228826310464] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:40 INFO 140228826310464] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:40 INFO 140228826310464] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:40 INFO 140228826310464] Loaded iterator creator application/json for content type ('application/json', '1.0')\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:40 INFO 140228826310464] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:40 INFO 140228826310464] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:41 WARNING 140228826310464] /opt/ml/input/data/train/train_annotation is not a readable image file\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:41 WARNING 140228826310464] label maps not provided, using defaults.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:41 INFO 140228826310464] #label_map train :{'scale': 1}\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:41 WARNING 140228826310464] /opt/ml/input/data/validation/validation_annotation is not a readable image file\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:41 WARNING 140228826310464] label maps not provided, using defaults.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:41 INFO 140228826310464] #label_map validation :{'scale': 1}\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:41 INFO 140228826310464] nvidia-smi took: 0.100751876831 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:41 INFO 140228826310464] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:52:41 INFO 140228826310464] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[01:52:44] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x.1160.0/AL2012/generic-flavor/src/src/storage/storage.cc:108: Using GPUPooledRoundedStorageManager.\u001b[0m\n",
      "\n",
      "2020-04-29 01:52:35 Training - Training image download completed. Training in progress.\u001b[34m[04/29/2020 01:52:49 INFO 140228826310464] LRScheduler setup: iters per epoch: 10, num_epochs 20\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1588125169.371802, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\"}, \"StartTime\": 1588125169.371747}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:03 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 0, train loss: 0.8510423302650452 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:03 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 0, train throughput: 17.203215351 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:05 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 0, validation pixel_accuracy: 0.7188921440972222 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:05 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 0, validation mIOU: 0.4146835884741791 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:05 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 0, validation throughput: 56.2136943937 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:05 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:05 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:06 INFO 140228826310464] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1588125186.104841, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 0}, \"StartTime\": 1588125169.372058}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:20 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 1, train loss: 0.22525509108196606 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:20 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 1, train throughput: 17.2269325121 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:23 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 1, validation pixel_accuracy: 0.8440256076388889 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:23 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 1, validation mIOU: 0.5313087918191981 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:23 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 1, validation throughput: 60.2070850162 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:23 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:23 INFO 140228826310464] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1588125203.425564, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 1}, \"StartTime\": 1588125186.105155}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/29/2020 01:53:37 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 2, train loss: 0.10858195945620537 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:37 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 2, train throughput: 17.5463143867 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:39 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 2, validation pixel_accuracy: 0.9598350694444444 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:39 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 2, validation mIOU: 0.7040224326012159 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:39 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 2, validation throughput: 61.2093255733 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:39 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:39 INFO 140228826310464] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1588125219.999076, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 2}, \"StartTime\": 1588125203.425919}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:55 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 3, train loss: 0.08036780831488696 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:55 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 3, train throughput: 17.4983417527 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:57 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 3, validation pixel_accuracy: 0.9685921223958334 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:57 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 3, validation mIOU: 0.7194434896667145 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:57 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 3, validation throughput: 61.5081473783 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:57 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:53:57 INFO 140228826310464] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1588125237.815566, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 3}, \"StartTime\": 1588125219.999423}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:12 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 4, train loss: 0.0707671120762825 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:12 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 4, train throughput: 17.3412850434 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:14 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 4, validation pixel_accuracy: 0.9708013237847222 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:14 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 4, validation mIOU: 0.7109044191842753 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:14 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 4, validation throughput: 61.5422448673 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:14 INFO 140228826310464] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1588125254.643891, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 4}, \"StartTime\": 1588125237.815888}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:28 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 5, train loss: 0.07161819487810135 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:28 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 5, train throughput: 17.445940715 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:30 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 5, validation pixel_accuracy: 0.9794753689236111 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:30 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 5, validation mIOU: 0.8243753989956619 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:30 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 5, validation throughput: 60.6298108392 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:30 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:30 INFO 140228826310464] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1588125270.990004, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 5}, \"StartTime\": 1588125254.644067}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:46 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 6, train loss: 0.06536841257051988 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:46 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 6, train throughput: 17.5244730735 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:48 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 6, validation pixel_accuracy: 0.9793798828125 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:48 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 6, validation mIOU: 0.8337394431370784 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:48 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 6, validation throughput: 61.2993975001 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:48 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:54:49 INFO 140228826310464] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1588125289.058519, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 6}, \"StartTime\": 1588125270.990374}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/29/2020 01:55:03 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 7, train loss: 0.06036086075685241 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:03 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 7, train throughput: 17.4202879077 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:06 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 7, validation pixel_accuracy: 0.9827164713541666 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:06 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 7, validation mIOU: 0.8454581702587733 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:06 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 7, validation throughput: 61.4904507681 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:06 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:06 INFO 140228826310464] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1588125306.642572, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 7}, \"StartTime\": 1588125289.058743}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:20 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 8, train loss: 0.05475303940474987 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:20 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 8, train throughput: 17.5280076106 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:22 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 8, validation pixel_accuracy: 0.9809901258680556 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:22 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 8, validation mIOU: 0.8155359245740001 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:22 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 8, validation throughput: 61.0121071671 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:22 INFO 140228826310464] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1588125322.826347, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 8}, \"StartTime\": 1588125306.64278}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:38 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 9, train loss: 0.05217945948243141 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:38 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 9, train throughput: 17.5556035312 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:40 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 9, validation pixel_accuracy: 0.9828011067708333 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:40 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 9, validation mIOU: 0.84563061599646 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:40 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 9, validation throughput: 60.8064295892 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:40 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:40 INFO 140228826310464] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1588125340.76375, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 9}, \"StartTime\": 1588125322.82654}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:54 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 10, train loss: 0.04777212403714657 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:54 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 10, train throughput: 17.544024522 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:57 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 10, validation pixel_accuracy: 0.9826898871527778 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:57 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 10, validation mIOU: 0.8314495290402932 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:57 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 10, validation throughput: 60.8507037937 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:55:57 INFO 140228826310464] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1588125357.082882, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 10}, \"StartTime\": 1588125340.76396}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:12 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 11, train loss: 0.0517236590385437 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:12 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 11, train throughput: 17.5470167867 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:14 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 11, validation pixel_accuracy: 0.9844070095486112 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:14 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 11, validation mIOU: 0.8545424532198395 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:14 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 11, validation throughput: 59.4672877859 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:14 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:15 INFO 140228826310464] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1588125375.159321, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 11}, \"StartTime\": 1588125357.083041}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:30 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 12, train loss: 0.05399190431291407 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:30 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 12, train throughput: 17.4104539965 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:32 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 12, validation pixel_accuracy: 0.9852446831597222 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:32 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 12, validation mIOU: 0.8624067712503567 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:32 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 12, validation throughput: 60.780655647 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:32 INFO 140228826310464] Serializing model to /opt/ml/model/model_best.params\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:33 INFO 140228826310464] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1588125393.166896, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 12}, \"StartTime\": 1588125375.15966}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/29/2020 01:56:46 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 13, train loss: 0.05154777355492115 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:46 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 13, train throughput: 17.5036904826 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:49 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 13, validation pixel_accuracy: 0.9843614366319444 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:49 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 13, validation mIOU: 0.859218594864106 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:49 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 13, validation throughput: 60.6975625595 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:56:49 INFO 140228826310464] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1588125409.261248, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 13}, \"StartTime\": 1588125393.167235}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:04 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 14, train loss: 0.04873502897945317 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:04 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 14, train throughput: 17.5885158754 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:07 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 14, validation pixel_accuracy: 0.9843207465277778 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:07 INFO 140228826310464] #quality_metric. host: algo-1, epoch: 14, validation mIOU: 0.8571004232354926 .\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:07 INFO 140228826310464] #throughput_metric. host: algo-1, epoch: 14, validation throughput: 58.3621059937 samples/sec.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:07 INFO 140228826310464] #early stop. best miou at best epoch (12): 0.8624067712503567, pix_acc 0.9852446831597222\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:07 INFO 140228826310464] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:07 INFO 140228826310464] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1588125427.235557, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\", \"epoch\": 14}, \"StartTime\": 1588125409.261421}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:07 WARNING 140228826310464] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:07 INFO 140228826310464] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/29/2020 01:57:07 INFO 140228826310464] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"totaltime\": {\"count\": 1, \"max\": 268147.7360725403, \"sum\": 268147.7360725403, \"min\": 268147.7360725403}, \"setuptime\": {\"count\": 1, \"max\": 20.203113555908203, \"sum\": 20.203113555908203, \"min\": 20.203113555908203}}, \"EndTime\": 1588125427.697748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Semantic Segmentation\"}, \"StartTime\": 1588125160.978862}\n",
      "\u001b[0m\n",
      "\n",
      "2020-04-29 01:57:40 Uploading - Uploading generated training model\n",
      "2020-04-29 01:58:47 Completed - Training job completed\n",
      "Training seconds: 463\n",
      "Billable seconds: 463\n"
     ]
    }
   ],
   "source": [
    "ss_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 部署与验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "ss_predictor = ss_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-067412632299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# import matplotlib.pyplot as plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_filename' is not defined"
     ]
    }
   ],
   "source": [
    "# import glob\n",
    "# import os\n",
    "# import shutil\n",
    "# import PIL\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import io\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "with open(test_filename, 'rb') as image:\n",
    "    img = image.read()\n",
    "    img = bytearray(img)\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "                        EndpointName='psp-seg-exp-aoyu-20200427-10-51-49-837',\n",
    "                        ContentType='image/jpeg',\n",
    "                        Accept='image/png',\n",
    "                        Body=img)\n",
    "# mask1 = np.array(Image.open(io.BytesIO(response['Body'].read())))\n",
    "# num_classes = 2\n",
    "# mask1 = np.fliplr(np.transpose(mask1))\n",
    "# plt.imshow(mask1, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化点：\n",
    "## 后处理步骤，增强分割的效果\n",
    "## 批处理方式，方便批量处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for patch transform\n",
    "# prepare data\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import glob\n",
    "from os import path as osp\n",
    "\n",
    "raw_path = 'raw'\n",
    "save_path = 'batch_test'\n",
    "height = 1520\n",
    "width = 1024\n",
    "\n",
    "for f in glob.glob1(raw_path, \"*.jpg\"):\n",
    "    im = PIL.Image.open(osp.join(raw_path,f))\n",
    "    im.thumbnail([height,width],PIL.Image.ANTIALIAS)\n",
    "    im.save(osp.join(save_path,f), \"JPEG\")\n",
    "    im2 = im.rotate(90, expand=True)\n",
    "    f2 = f.split('.')[0]+'_r.jpg'\n",
    "    im2.save(osp.join(save_path,f2), \"JPEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import boto3\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "\n",
    "def liaoning_test(dirr):\n",
    "    prefix = '.'\n",
    "    train_prefix = os.path.join(prefix,'train')\n",
    "    val_prefix = os.path.join(prefix,'validation')\n",
    "    test_prefix = os.path.join(prefix,'test')\n",
    "    dir_name_output = 'batch_test_result_instance'\n",
    "    \n",
    "    os.makedirs(train_prefix, exist_ok=True)\n",
    "    os.makedirs(val_prefix, exist_ok=True)\n",
    "    os.makedirs(test_prefix, exist_ok=True)\n",
    "    # dir_list = ['train','validation','test']\n",
    "    dir_list = ['batch_test']\n",
    "    for dirr in dir_list:\n",
    "        for filename in glob.glob1(dirr,\"*.jpg\"):\n",
    "            %%time\n",
    "            if filename.split('.')[0][-2:] == '_r':\n",
    "                continue\n",
    "            #         test_filename = filename.split('.')[0]+'_r.jpg'\n",
    "            #         test_filename = Path(os.path.join(dirr,test_filename))\n",
    "            #         if test_filename.exists():\n",
    "            #             continue\n",
    "            # resize image size for inference\n",
    "            filename = os.path.join(dirr,filename)\n",
    "            im1 = PIL.Image.open(filename)\n",
    "            im1.thumbnail([800,600],PIL.Image.ANTIALIAS)\n",
    "            #         im1.thumbnail([1520,1024],PIL.Image.ANTIALIAS)\n",
    "            im1.save(filename, \"JPEG\")\n",
    "            ff = filename.split('/')[1]\n",
    "            save_file = os.path.join(prefix,dirr,ff)\n",
    "            #         print(save_file)\n",
    "            with open(filename, 'rb') as image:\n",
    "                img = image.read()\n",
    "                img = bytearray(img)\n",
    "\n",
    "            #         ss_predictor.content_type = 'image/jpeg'\n",
    "            #         ss_predictor.accept = 'image/png'\n",
    "            #         results = ss_predictor.predict(img)\n",
    "            response = runtime_sm_client.invoke_endpoint(\n",
    "                            EndpointName='ss-notebook-demo-2020-04-29-01-48-56-769',\n",
    "                            ContentType='image/jpeg',\n",
    "                            Accept='image/png',\n",
    "                            Body=img)\n",
    "            mask = np.array(Image.open(io.BytesIO(response['Body'].read())))\n",
    "            num_classes = 2\n",
    "            #         mask = np.array(Image.open(io.BytesIO(results)))\n",
    "            #         plt.imshow(mask, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "            #         plt.savefig(save_file, bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            if dirr == 'batch_test':\n",
    "                im2 = PIL.Image.open(filename)\n",
    "                im2 = im2.rotate(90, expand=True)\n",
    "                im2.thumbnail([600,800],PIL.Image.ANTIALIAS)\n",
    "            #             im2.thumbnail([1024,1520],PIL.Image.ANTIALIAS)\n",
    "                rfilename = filename.split('.')[0]+'_r.jpg'\n",
    "                im2.save(rfilename, \"JPEG\")\n",
    "                with open(rfilename, 'rb') as image:\n",
    "                    img = image.read()\n",
    "                    img = bytearray(img)\n",
    "            #             rresults = ss_predictor.predict(img)\n",
    "                response = runtime_sm_client.invoke_endpoint(\n",
    "                            EndpointName='ss-notebook-demo-2020-04-29-01-48-56-769',\n",
    "                            ContentType='image/jpeg',\n",
    "                            Accept='image/png',\n",
    "                            Body=img)\n",
    "                rmask = np.array(Image.open(io.BytesIO(response['Body'].read()))) \n",
    "            #             rmask = np.array(Image.open(io.BytesIO(rresults)))\n",
    "            #             rsave_file = os.path.join(prefix,dirr,ff.split('.')[0]+\"_r.jpg\")\n",
    "            #             plt.imshow(rmask, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "            #             plt.savefig(rsave_file, bbox_inches='tight')\n",
    "                rmask = np.fliplr(np.transpose(rmask))|mask\n",
    "                ret, binary = cv2.threshold(rmask*255,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "            #             print(dir_name_raw)\n",
    "                contours, hierarchy = cv2.findContours(binary,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                raw_img = cv2.imread(filename)\n",
    "                ffresult = raw_img\n",
    "                contours_poly = [None]*len(contours)\n",
    "                boundRect = [None]*len(contours)\n",
    "                for i, c in enumerate(contours):\n",
    "                    if c.size < contour_size_limit:\n",
    "                        continue\n",
    "                contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "                boundRect[i] = cv2.boundingRect(contours_poly[i])\n",
    "                cv2.rectangle(ffresult, (int(boundRect[i][0]), int(boundRect[i][1])), \\\n",
    "                              (int(boundRect[i][0]+boundRect[i][2]), int(boundRect[i][1]+boundRect[i][3])), color, 2)\n",
    "                #fill contours\n",
    "                colormask = np.zeros(ffresult.shape,np.uint8)\n",
    "                cv2.fillPoly(colormask, pts = [c], color = color)\n",
    "                ffresult = cv2.addWeighted(ffresult, 1.0, colormask, 0.2, 0)\n",
    "                cv2.imwrite(osp.join(dir_name_output,filename.split('/')[-1]), ffresult)\n",
    "#                 plt.imshow(ffresult)\n",
    "                print(\"finish process file {} !!!!!!!!!\".format(filename.split('/')[-1]))\n",
    "    return 1\n",
    "\n",
    "\n",
    "\n",
    "#             fmask = np.fliplr(np.transpose(rmask))|mask\n",
    "# #             ret, binary = cv2.threshold(fmask*255,127,255,cv2.THRESH_BINARY)\n",
    "# #             contours, hierarchy = cv2.findContours(binary,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "#             raw_img = cv2.imread(filename)\n",
    "# #             cv2.drawContours(raw_img,contours,-1,(0,0,255),3)\n",
    "#             fsave_file = os.path.join(prefix,dirr,ff.split('.')[0]+\"_f.jpg\")\n",
    "# #             cv2.imwrite(fsave_file, raw_img)\n",
    "#             colorfmask = np.zeros([fmask.shape[0],fmask.shape[1],3],np.uint8)\n",
    "#             colorfmask[:,:,0] = np.zeros(fmask.shape)\n",
    "#             colorfmask[:,:,1] = fmask*255\n",
    "#             colorfmask[:,:,2] = np.zeros(fmask.shape)\n",
    "#             ffresult = cv2.addWeighted(raw_img, 1.0, colorfmask, 0.01, 1.0)\n",
    "#             cv2.imwrite(fsave_file, ffresult)\n",
    "#             plt.imshow(fmask, vmin=0, vmax=num_classes-1, cmap='jet')\n",
    "#             plt.savefig(fsave_file, bbox_inches='tight')\n",
    "#print ( ' Num Train Images = ' + str(num_training_samples))\n",
    "#assert num_training_samples == len(glob.glob1('train_annotation',\"*.png\"))\n",
    "\n",
    "#print ( ' Num Validation Images = ' + str(len(glob.glob1('validation',\"*.jpg\"))))\n",
    "#assert len(glob.glob1('validation',\"*.jpg\")) == len(glob.glob1('validation_annotation',\"*.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简化脚本，方便客户评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "liaoning_test(dirr = \"./raw\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
